% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/PredVal.R
\name{PredVal}
\alias{PredVal}
\title{Predictive Values of Each Base Learner in Each Data Set}
\usage{
PredVal(models, TestSet, resp.var, ref.lv = NULL, method = "none",
  metric = NULL, stack.wt = NULL, trControl = NULL,
  tuneLength = NULL)
}
\arguments{
\item{ref.lv}{reference level for categorical variables.}

\item{method}{A string specifying which classification or
regression model to use. Possible values are found using
\code{names(getModelInfo())}. See
\url{http://topepo.github.io/caret/train-models-by-tag.html}. A
list of functions can also be passed for a custom model
function. See
\url{http://topepo.github.io/caret/using-your-own-model-in-train.html}
for details.}

\item{metric}{A string that specifies what summary metric will
be used to select the optimal model. By default, possible values
are "RMSE" and "Rsquared" for regression and "Accuracy" and
"Kappa" for classification. If custom performance metrics are
used (via the \code{summaryFunction} argument in
\code{\link{trainControl}}, the value of \code{metric} should
match one of the arguments. If it does not, a warning is issued
and the first metric given by the \code{summaryFunction} is
used. (NOTE: If given, this argument must be named.)}

\item{stack.wt}{???}

\item{trControl}{A list of values that define how this function
acts. See \code{\link{trainControl}} and
\url{http://topepo.github.io/caret/using-your-own-model-in-train.html}.
(NOTE: If given, this argument must be named.)}

\item{tuneLength}{An integer denoting the amount of granularity
in the tuning parameter grid. By default, this argument is the
number of levels for each tuning parameters that should be
generated by \code{\link{train}}. If \code{\link{trainControl}}
has the option \code{search = "random"}, this is the maximum
number of tuning parameter combinations that will be generated
by the random search. (NOTE: If given, this argument must be
named.)}
}
\description{
To assess model performance in the training and testing sets, we need:
\enumerate{
\item Predictived values of each base learner in each data set.
\item If a stack model is build on the top of individual base learners, then we also need the predictive values of the stack model in both sets.
\item Compute performance metrics.
}
}
\details{
For consistency purpose (with stacking predictions), I use \code{defaultSummary(pred)}

In PredVal, you can
\enumerate{
\item specify a stacking method
\item specify a weight for each ML algorithm
}
}
